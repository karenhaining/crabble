{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "685d4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import ConcatDataset\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bd337ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "class CustomScrabbleDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='training', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to dataset root.\n",
    "            split (str): 'training' or 'val'.\n",
    "            transform (callable, optional): Optional transform to apply to images.\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.transform = transform\n",
    "\n",
    "        # Each class folder (A, B, ...)\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name, split)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[class_name] = idx\n",
    "\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append(os.path.join(class_path, fname))\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load the image using PIL\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "# def split_clean_and_augmented(dataset, val_ratio=0.2, exclude_token=\"augmented\"):\n",
    "#     clean_indices = []\n",
    "#     augmented_indices = []\n",
    "\n",
    "#     for i, (path, label) in enumerate(dataset.samples):\n",
    "#         if exclude_token in os.path.normpath(path).split(os.sep):\n",
    "#             augmented_indices.append(i)\n",
    "#         else:\n",
    "#             clean_indices.append(i)\n",
    "\n",
    "#     # Shuffle and split the clean data into train/val\n",
    "#     random.shuffle(clean_indices)\n",
    "#     val_size = int(val_ratio * len(clean_indices))\n",
    "#     val_indices = clean_indices[:val_size]\n",
    "#     train_indices = clean_indices[val_size:] + augmented_indices  # Include augmented only in train\n",
    "\n",
    "#     return Subset(dataset, train_indices), Subset(dataset, val_indices)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load all images (ImageFolder will recurse into subfolders)\n",
    "# dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "\n",
    "# # Perform the smart split\n",
    "# train_dataset, val_dataset = split_clean_and_augmented(dataset, val_ratio=0.2)\n",
    "\n",
    "# # DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_dataset = CustomScrabbleDataset('data', split='training', transform=transform)\n",
    "val_dataset = CustomScrabbleDataset('data', split='val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "    'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "    'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
    "    'V', 'W', 'X', 'Y', 'Z'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f0725dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, len(class_names))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "715436ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3934\n",
      "Epoch 2, Loss: 1.3578\n",
      "Epoch 3, Loss: 1.0229\n",
      "Epoch 4, Loss: 0.7932\n",
      "Epoch 5, Loss: 0.6501\n",
      "Epoch 6, Loss: 0.4928\n",
      "Epoch 7, Loss: 0.3781\n",
      "Epoch 8, Loss: 0.2758\n",
      "Epoch 9, Loss: 0.1833\n",
      "Epoch 10, Loss: 0.1224\n",
      "Epoch 11, Loss: 0.0752\n",
      "Epoch 12, Loss: 0.0452\n",
      "Epoch 13, Loss: 0.0251\n",
      "Epoch 14, Loss: 0.0147\n",
      "Epoch 15, Loss: 0.0094\n",
      "Epoch 16, Loss: 0.0066\n",
      "Epoch 17, Loss: 0.0046\n",
      "Epoch 18, Loss: 0.0039\n",
      "Epoch 19, Loss: 0.0032\n",
      "Epoch 20, Loss: 0.0027\n",
      "Epoch 21, Loss: 0.0023\n",
      "Epoch 22, Loss: 0.0020\n",
      "Epoch 23, Loss: 0.0018\n",
      "Epoch 24, Loss: 0.0016\n",
      "Epoch 25, Loss: 0.0014\n",
      "Epoch 26, Loss: 0.0013\n",
      "Epoch 27, Loss: 0.0012\n",
      "Epoch 28, Loss: 0.0011\n",
      "Epoch 29, Loss: 0.0010\n",
      "Epoch 30, Loss: 0.0009\n",
      "Epoch 31, Loss: 0.0008\n",
      "Epoch 32, Loss: 0.0008\n",
      "Epoch 33, Loss: 0.0007\n",
      "Epoch 34, Loss: 0.0007\n",
      "Epoch 35, Loss: 0.0006\n",
      "Epoch 36, Loss: 0.0006\n",
      "Epoch 37, Loss: 0.0005\n",
      "Epoch 38, Loss: 0.0005\n",
      "Epoch 39, Loss: 0.0005\n",
      "Epoch 40, Loss: 0.0004\n",
      "Epoch 41, Loss: 0.0004\n",
      "Epoch 42, Loss: 0.0004\n",
      "Epoch 43, Loss: 0.0004\n",
      "Epoch 44, Loss: 0.0003\n",
      "Epoch 45, Loss: 0.0003\n",
      "Epoch 46, Loss: 0.0003\n",
      "Epoch 47, Loss: 0.0003\n",
      "Epoch 48, Loss: 0.0003\n",
      "Epoch 49, Loss: 0.0003\n",
      "Epoch 50, Loss: 0.0002\n",
      "Epoch 51, Loss: 0.0002\n",
      "Epoch 52, Loss: 0.0002\n",
      "Epoch 53, Loss: 0.0002\n",
      "Epoch 54, Loss: 0.0002\n",
      "Epoch 55, Loss: 0.0002\n",
      "Epoch 56, Loss: 0.0002\n",
      "Epoch 57, Loss: 0.0002\n",
      "Epoch 58, Loss: 0.0002\n",
      "Epoch 59, Loss: 0.0002\n",
      "Epoch 60, Loss: 0.0001\n",
      "Epoch 61, Loss: 0.0001\n",
      "Epoch 62, Loss: 0.0001\n",
      "Epoch 63, Loss: 0.0001\n",
      "Epoch 64, Loss: 0.0001\n",
      "Epoch 65, Loss: 0.0001\n",
      "Epoch 66, Loss: 0.0001\n",
      "Epoch 67, Loss: 0.0001\n",
      "Epoch 68, Loss: 0.0001\n",
      "Epoch 69, Loss: 0.0001\n",
      "Epoch 70, Loss: 0.0001\n",
      "Epoch 71, Loss: 0.0001\n",
      "Epoch 72, Loss: 0.0001\n",
      "Epoch 73, Loss: 0.0001\n",
      "Epoch 74, Loss: 0.0001\n",
      "Epoch 75, Loss: 0.0001\n",
      "Epoch 76, Loss: 0.0001\n",
      "Epoch 77, Loss: 0.0001\n",
      "Epoch 78, Loss: 0.0001\n",
      "Epoch 79, Loss: 0.0001\n",
      "Epoch 80, Loss: 0.0001\n",
      "Epoch 81, Loss: 0.0001\n",
      "Epoch 82, Loss: 0.0001\n",
      "Epoch 83, Loss: 0.0001\n",
      "Epoch 84, Loss: 0.0001\n",
      "Epoch 85, Loss: 0.0000\n",
      "Epoch 86, Loss: 0.0000\n",
      "Epoch 87, Loss: 0.0000\n",
      "Epoch 88, Loss: 0.0000\n",
      "Epoch 89, Loss: 0.0000\n",
      "Epoch 90, Loss: 0.0000\n",
      "Epoch 91, Loss: 0.0000\n",
      "Epoch 92, Loss: 0.0000\n",
      "Epoch 93, Loss: 0.0000\n",
      "Epoch 94, Loss: 0.0000\n",
      "Epoch 95, Loss: 0.0000\n",
      "Epoch 96, Loss: 0.0000\n",
      "Epoch 97, Loss: 0.0000\n",
      "Epoch 98, Loss: 0.0000\n",
      "Epoch 99, Loss: 0.0000\n",
      "Epoch 100, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LetterClassifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34e49beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.16%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b0940c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img * 0.5 + 0.5  # unnormalize\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = next(dataiter)\n",
    "outputs = model(images.to(device))\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "# Show first 6 images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for idx in range(6):\n",
    "    plt.subplot(1, 6, idx+1)\n",
    "    imshow(images[idx])\n",
    "    plt.title(f\"Pred: {class_names[preds[idx]]}\\nTrue: {class_names[labels[idx]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f6d116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'letter_classifier.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
